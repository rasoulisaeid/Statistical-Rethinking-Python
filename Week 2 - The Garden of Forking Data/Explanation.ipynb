{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What proportion of the surface is covered with water?\n",
    "--------------------\n",
    "### Workflow:\n",
    "1. Define generative model of the sample\n",
    "2. Define a specific estimand\n",
    "3. Design a statistical way to produce estimate\n",
    "4. Test (3) using (1)\n",
    "5. Analyze sample and summarize\n",
    "--------------------\n",
    "### 1. Generative model of the globe\n",
    "#### Consider tossing a 4-sided globe with 25% water. Every time our finger touches the globe, we write down if it's water(W) or land(L).\n",
    "- Proportion of water: `P` => It influences W and L\n",
    "- Number of tosses: N => It's under our control and influences W and L \n",
    "- Count of all water observations: W => It depends on P and also number of times we toss the globe(N)\n",
    "- Count of all land observations: L => It depends on P and also number of times we toss the globe(N)\n",
    "\n",
    "<img src=\"images/image0.png\" width=\"300\" height=\"200\" />\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Estimand and 3. Estimate\n",
    "#### If we toss the globe 3 times, how many ways is it possible to observe `W L W`?\n",
    "\n",
    "<img src=\"images/3_times_toss.png\" width=\"500\" height=\"300\" />\n",
    "\n",
    "#### How about if the proportion of water be 75%?\n",
    "\n",
    "<img src=\"images/75_percent.png\" width=\"500\" height=\"300\" />\n",
    "\n",
    "#### All ways:\n",
    "\n",
    "<img src=\"images/all_ways.png\" width=\"400\" height=\"200\" /> <img src=\"images/counts_of_all.png\" width=\"500\" height=\"300\" />\n",
    "\n",
    "#### What if we want to toss one more time and want to update our counts?\n",
    "\n",
    "<img src=\"images/updating.png\" width=\"500\" height=\"300\" />\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Whole sample if we toss the globe 9 times:\n",
    "\n",
    "<img src=\"images/image1.png\" width=\"600\" height=\"400\" />\n",
    "\n",
    "<img src=\"images/image2.png\" width=\"600\" height=\"400\" />\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test Before You Est(imate)\n",
    "\n",
    "- Code a generative stimulation\n",
    "- Code an estimator\n",
    "- Test the estimator with the stimulation\n",
    "\n",
    "<img src=\"images/image4.png\" width=\"600\" height=\"400\" /> <img src=\"images/image6.png\" width=\"600\" height=\"400\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When we test the estimator where the answer is known, we can see that as the sample size increases it converges to the right answer and when the sample size is small it correctly characterizes the uncertainty.\n",
    "\n",
    "<img src=\"images/image7.png\" width=\"600\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When the possibilities increase, the maximum probability decreases. However the sum of probabilities is still 1. The bars get shorter as we move from left to right and that's because we have more possibilities and there is less possibility in each bar.\n",
    "\n",
    "<img src=\"images/more_possibilities.png\" width=\"400\" height=\"250\" /> <img src=\"images/image8.png\" width=\"600\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/image9.png\" width=\"600\" height=\"400\" /> <img src=\"images/image10.png\" width=\"600\" height=\"400\" />\n",
    "<img src=\"images/image11.png\" width=\"600\" height=\"400\" /> <img src=\"images/image12.png\" width=\"600\" height=\"400\" />\n",
    "\n",
    "#### The minimum sample size is 1 in bayesian inference. Although this 1 sample is not very informative but it's part of the power of this. It accurately represent the relative confidence we could assign to each of possible proportion of water."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/image13.png\" width=\"600\" height=\"400\" /> <img src=\"images/image14.png\" width=\"600\" height=\"400\" />\n",
    "\n",
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/image15.png\" width=\"800\" height=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Predictive Distribution:\n",
    "Is a prediction for a future experimental observation made from our existing estimate. So we can say given what we have learned about the globe so far what would happen if we took more samples from it?\n",
    "> We will probably observe that if we take 10 samples from the distribution, for the most of the time it will hang around 0.6 where the posterior probability is higher.\n",
    "\n",
    "<img src=\"images/image16.png\" width=\"600\" height=\"400\" />\n",
    "\n",
    "If we take a sample from a region shown in the leftmost plot, its predictive distribution will resemble the middle plot. This means that if we toss the globe 9 times, given that the proportion of water around that chosen point is near 0.7, the predictive distribution indicates a *central tendency of 6 water samples out of 9*.\n",
    "\n",
    "<img src=\"images/image17.png\" width=\"600\" height=\"400\" />\n",
    "\n",
    "In this example, the proportion of water is about 0.4, and the predictive distribution is skewed towards lower water sample sizes, indicating a *central tendency of 3 water samples out of 9*.\n",
    "\n",
    "> Why is the posterior predictive distribution flatter and more spread out than the predictive distributions?\n",
    ">> This is because it contains samples from all the predictive distributions. It accurately characterizes the uncertainty in our estimate about what will happen if we do the experiment again. For each position, it indicates all the times that a certain number of water samples has been observed.\n",
    "\n",
    "<img src=\"images/image19.png\" width=\"600\" height=\"400\" /> <img src=\"images/image20.png\" width=\"600\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/image21.png\" width=\"600\" height=\"400\" /> \n",
    "\n",
    "> `The Logic of Bayesian Inference`\n",
    ">> Bayesian inference gives you no guarantees except that it’s logical. That is, it honestly carries out the assumptions you put into it, and it does the best possible job that it possibly can, taking those assumptions seriously.\n",
    "\n",
    "> `The Role of Bayesian Inference`\n",
    ">> And this doesn’t tell us what really happens in the world, but it **helps us tremendously to figure out what happens in the world**, because it lets us work with logical implications of assumptions and then compare those with data and do a **theory development loop** through that action. So this is *a quantitative asset that activates our qualitative knowledge as scientists*.\n",
    "\n",
    "> `The Subjectivity and Objectivity of Bayesian Inference`\n",
    ">> It lets the subjective and objective work together. Bayesian inference is completely objective, but the inputs into it, like in all statistical procedures, whether Bayesian or not, are subjective. And that’s good, because subjectivity is expertise.\n",
    "\n",
    "> `The Power of Bayesian Inference`\n",
    ">> Now we revise our beliefs when new things happen, and then we change our subjective beliefs, and then we activate the objective procedure again, and we see what the implications are of those changes in our subjective beliefs. And this is a powerful way to do inference. And we say any framework that is giving you more than this is hiding some assumptions that is letting it claim those more things. And so you could always make those assumptions explicit in the Bayesian framework and do the same thing as that other framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
