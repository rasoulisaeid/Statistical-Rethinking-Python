{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helsinki: A Case Study\n",
    "\n",
    "- **Helsinki** is the capital of **Finland** and a pretty cold place.\n",
    "- Despite the cold, the Finnish people are the **happiest in the world**.\n",
    "- Among the many great things about Finland is **metal music**:\n",
    "  - Finland has more heavy metal bands per person than any other country.\n",
    "  - When you plot **happiness** and the **logarithm of the number of metal bands per million people** on the same graph, Finland ranks number one in both.\n",
    "\n",
    "<img src=\"images/image1.jpeg\" width=\"600\" height=\"400\" />\n",
    "\n",
    "---\n",
    "\n",
    "### Correlations vs. Causation\n",
    "\n",
    "- **Spurious correlations** are common. Examples include:\n",
    "  - The number of **Waffle Houses** and the **divorce rate** in the Southern United States.\n",
    "  - There is a strong statistical association between the number of Waffle Houses per million people and the divorce rate, but it's not plausible that Waffle House causes divorce any more than it's plausible that heavy metal makes nations happy.\n",
    "\n",
    " <img src=\"images/image2.jpeg\" width=\"600\" height=\"400\" />\n",
    "\n",
    "- **Time Series Correlations**:\n",
    "  - Example: The divorce rate in **Maine** from 2000 to 2009 correlated with the per capita consumption of **margarine** in the same years, with a correlation of 0.99.\n",
    "\n",
    " <img src=\"images/image3.jpeg\" width=\"600\" height=\"400\" />\n",
    " \n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Correlation is common in nature; causation is sparse**.\n",
    "- We must be sophisticated about how we think about threats to validity to distinguish associations that reflect cause from those that don't.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remind you where we're going in this course, in every example, the idea is we start with a scientific question and demand, which is our goal. Here is a cooking metaphor:\n",
    "\n",
    "- **Goal**: Make a hedgehog cake (our demand).\n",
    "- **Recipe**: Our estimator (a set of instructions to assemble ingredients like data and code).\n",
    "- **Result**: Our estimate (the outcome that resembles the demand).\n",
    "\n",
    "<img src=\"images/image4.jpeg\" width=\"600\" height=\"400\" />\n",
    "\n",
    "Often, things can go wrong in either the design of the estimator or in its usage, resulting in an estimate that may not be what we hoped for. Today, we'll discuss these kinds of threats, specifically those mismatches between the estimator and the process we're studying that can lead us astray. This topic is sometimes called **confounding**.\n",
    "\n",
    "<img src=\"images/image5.jpeg\" width=\"600\" height=\"400\" />\n",
    "\n",
    "## Understanding Confounding\n",
    "\n",
    "The term **confound** means many things in statistics, but we'll use it in its plain English sense: something that misleads us. When confounded, you're confused. The causal sources of confounds are diverse, and today's goal is to introduce you to **the four elemental confounds**. Despite the diversity of causes, they are all built up from four fundamental relationships between triplets of variables: the fork, the pipe, the collider, and the descendant.\n",
    "\n",
    "<img src=\"images/image6.jpeg\" width=\"600\" height=\"400\" />\n",
    "\n",
    "### The Fork\n",
    "\n",
    "The fork is the most fundamental relationship, often introduced first to statistics students. It involves three variables: \\(X\\), \\(Y\\), and \\(Z\\). We're interested in the association between \\(X\\) and \\(Y\\) and analyzing the role of \\(Z\\) in influencing that relationship.\n",
    "\n",
    "- **Common Cause**: \\(Z\\) is a common cause of \\(X\\) and \\(Y\\). \\(X\\) and \\(Y\\) will be associated because they share \\(Z\\), their common cause.\n",
    "- **Notation**: \\(Y \\nind X\\) means \\(Y\\) is not independent of \\(X\\). Knowing \\(Y\\) or \\(X\\) tells us something about the other due to their shared information from \\(Z\\).\n",
    "- **Stratification**: When stratifying the sample by \\(Z\\), \\(X\\) and \\(Y\\) become independent at each level of \\(Z\\). We write this as \\(Y \\ind X \\mid Z\\).\n",
    "\n",
    "<img src=\"images/image7.jpeg\" width=\"600\" height=\"400\" />\n",
    "\n",
    "#### Graphical Representation\n",
    "\n",
    "- **DAG (Directed Acyclic Graph)**: \\(Z\\) influences both \\(X\\) and \\(Y\\).\n",
    "- **Particles of Influence**: Represented by filled and empty circles, indicating different values.\n",
    "- **Extra Influences**: \\(X\\) and \\(Y\\) also have their own unique influences, not drawn in the DAG.\n",
    "\n",
    "#### Simulation Example\n",
    "\n",
    "Here's a simple simulation of the fork:\n",
    "\n",
    "1. **Simulate \\(Z\\)**: Generate \\(Z\\) first.\n",
    "2. **Simulate \\(X\\) and \\(Y\\)**: Generate \\(X\\) and \\(Y\\) based on \\(Z\\) with additional random variation.\n",
    "\n",
    "\n",
    "<img src=\"images/image8.jpeg\" width=\"600\" height=\"400\" />\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis Example: Marriage Rates and Divorce Rates\n",
    "\n",
    "Simply looking at a scatter plot often doesn't reveal the underlying causal structure. We use causal models to design estimators that help us infer what's truly happening.\n",
    "\n",
    "#### Example: Marriage Rate and Divorce Rate\n",
    "\n",
    "Consider the relationship between marriage rates and divorce rates across the states of the United States. Higher marriage rates are statistically associated with higher divorce rates. This could imply:\n",
    "- A causal relationship.\n",
    "- Cultural factors influencing both.\n",
    "- The simple fact that people can only get divorced if they get married first.\n",
    "\n",
    "However, there might be more to this relationship. Our goal is to investigate the causal effect of marriage rates on divorce rates across regions in the U.S.\n",
    "\n",
    "#### Introducing Another Variable: Median Age at Marriage\n",
    "\n",
    "Another variable strongly related to divorce rates is the median age at which people get married. The relationship works in the opposite direction:\n",
    "- Higher marriage rates are associated with higher divorce rates.\n",
    "- Lower median ages of marriage are associated with higher divorce rates.\n",
    "\n",
    "#### Constructing a Causal Model\n",
    "\n",
    "We can model these relationships using a Directed Acyclic Graph (DAG):\n",
    "- **Median Age at Marriage**: A common cause (fork) affecting both marriage rates and divorce rates.\n",
    "  - Younger populations tend to have higher marriage rates.\n",
    "  - We need to assess whether the association between marriage rate and divorce rate is solely due to this common cause.\n",
    "\n",
    "#### Developing an Estimator\n",
    "\n",
    "We aim to develop a scientific model and statistical estimator to analyze the data. This approach helps us determine if the observed association between marriage and divorce rates results from their shared common cause (median age at marriage) or if there is a direct causal effect.\n",
    "\n",
    "#### Visualizing the Relationships\n",
    "\n",
    "Scatter plots alone cannot reveal the causal structure. The true causal relationships require knowledge of the variables and their interactions:\n",
    "- We need to think about the directions of the arrows in the DAG.\n",
    "- Multiple DAGs could fit the same scatter plots if the variables were anonymous.\n",
    "\n",
    "<img src=\"images/image10.jpeg\" width=\"800\" height=\"500\" />\n",
    "\n",
    "Understanding and modeling the causal relationships are essential for accurate data analysis and interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking the Fork: Estimating Causal Effects\n",
    "\n",
    "To understand causal effects in our model, we need to generate a synthetic simulation and develop an estimator. In this example, we'll skip the testing stage to focus on more complex aspects.\n",
    "\n",
    "#### Stratifying to Break the Fork\n",
    "\n",
    "To estimate the causal effect of \\( M \\) (marriage rate), we need to \"break the fork\" by stratifying by the common cause \\( A \\) (age at marriage). By stratifying, we remove the association between \\( M \\) and \\( D \\) (divorce rate) caused by \\( A \\). Here's how:\n",
    "\n",
    "- **Stratify by \\( A \\)**: Within each level of \\( A \\), the association between \\( M \\) and \\( D \\) caused by \\( A \\) is removed.\n",
    "- **Average Across Levels of \\( A \\)**: This helps isolate the direct influence of \\( M \\) on \\( D \\).\n",
    "\n",
    "<img src=\"images/image11.jpeg\" width=\"600\" height=\"400\" />\n",
    "\n",
    "#### Stratifying by a Continuous Variable\n",
    "\n",
    "When \\( A \\) is continuous (like age at marriage), stratifying means examining the association between \\( M \\) and \\( D \\) for every value of \\( A \\). We use a function to capture these relationships:\n",
    "\n",
    "- **Linear Regression**: A linear regression model can effectively stratify by incorporating \\( A \\) into the intercept. For each \\( A \\), we estimate a different relationship between \\( M \\) and \\( D \\).\n",
    "\n",
    "<img src=\"images/image12.jpeg\" width=\"600\" height=\"400\" />\n",
    "\n",
    "#### Example Linear Model\n",
    "\n",
    "Consider a regression model where the expected divorce rate (\\( E[D] \\)) is:\n",
    "\n",
    "<img src=\"images/image13.jpeg\" width=\"600\" height=\"400\" /> <img src=\"images/image14.jpeg\" width=\"600\" height=\"400\" />\n",
    "\n",
    "- \\( \\alpha \\): Intercept\n",
    "- \\( \\beta_M \\): Slope for marriage rate\n",
    "- \\( \\beta_A \\): Slope for age at marriage\n",
    "\n",
    "This model stratifies by \\( A \\), making \\( \\alpha + \\beta_A A \\) the intercept, and allows us to measure \\( \\beta_M \\) against this stratified baseline.\n",
    "\n",
    "#### Standardizing Variables\n",
    "\n",
    "Standardizing variables makes the mean zero and the standard deviation one, facilitating the development of priors and improving computational efficiency. Standardizing involves:\n",
    "\n",
    "- Subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "<img src=\"images/image15.jpeg\" width=\"600\" height=\"400\" /> <img src=\"images/image16.jpeg\" width=\"600\" height=\"400\" />\n",
    "\n",
    "#### Developing Priors\n",
    "\n",
    "For Bayesian models, selecting appropriate priors is crucial. Here’s a guideline for standardized variables:\n",
    "\n",
    "- **Intercept (\\( \\alpha \\))**: Mean of zero, reflecting the average divorce rate.\n",
    "- **Slopes (\\( \\beta_M \\), \\( \\beta_A \\))**: Should be loose enough to allow for realistic variations but not so extreme that they imply implausible relationships.\n",
    "\n",
    "We simulate prior predictive distributions to ensure these priors make sense.\n",
    "\n",
    "#### Simulation Example\n",
    "\n",
    "Simulating from \\( \\text{Normal}(0, 10) \\) priors can result in extreme slopes, which are unrealistic. More realistic priors might be \\( \\text{Normal}(0, 1) \\), allowing for both strong and weak associations without implausible extremes.\n",
    "\n",
    "<img src=\"images/image18.jpeg\" width=\"600\" height=\"400\" /> <img src=\"images/image19.jpeg\" width=\"600\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "look at the summary table and already get a sense of what's going on here we've got an intercept and two slopes and what i'm showing you in this plot this is the pricey plot this is a sometimes called a forest plot or caterpillar plot these are the posterior means in the open circles for each unknown in the posterior distribution and the bars are 89 percent percentile intervals or compatibility intervals i sometimes call them you can get a sense what's going on unsurprisingly the intercept alpha is centered on zero all right it has to be by measurement we induce that through the transformation and uh the two slopes are the focus of our interest you can see that um beta sub m bm there is close to zero and it spans both sides of it so any causal effect of marriage rate is is this doesn't say the causal effect of marriage is zero just because the interval includes zero i'll say that again this does not mean that the causal effect of marriage rate is zero just because this interval includes zero it's just as true that it could be negative right because the bar extends pretty far negative as well now zero is not a special point that annihilates all other values okay however you'll see that that compatibility interval is always closer to zero than beta sub a is or ba which is quite negative and nowhere close to zero there is about the same level of uncertainty in both of these slopes though the question we asked our estimated is what's the causal effect of m and often people will simply report the slope as the causal effect of m and that's not that's not terrible but it's not actually right or let's say it's not terrible because in a perfectly linear model after you do the right thing often you get an estimate that looks basically like the posterior distribution of the slope but that's only a feature of these really simple linear regressions and even slightly more complicated models and especially in non-linear models it is never true that the causal effect we're after the s demand is is a function of only one parameter of  only one unknown in the posterior distribution\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Open and read the content of the transcript file\n",
    "with open(\"./transcript.txt\") as f:\n",
    "    txt = f.read()\n",
    "    txt = txt.lower()\n",
    "    \n",
    "    # Find the index of the phrase \"associations that reflect cause\"\n",
    "    start_phrase = \"what we need to do surprize is\"\n",
    "    start_index = txt.find(start_phrase)\n",
    "    end_phrase = \"only one unknown in the posterior distribution\"\n",
    "    end_index = txt.find(end_phrase)\n",
    "    \n",
    "    sliced_txt = txt[start_index:end_index] + f\" {end_phrase}\"\n",
    "    \n",
    "    print(sliced_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
